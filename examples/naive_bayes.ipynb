{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3235802,"sourceType":"datasetVersion","datasetId":1961542}],"dockerImageVersionId":30527,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"> Edit: This notebook will be updated soon with a few more topics and references. Please do consider upvoting if you found it useful!","metadata":{}},{"cell_type":"markdown","source":"# Author's Note\n\n**Hello everyone üëãüèª,**\n\nWelcome to a beginner-friendly guide to Naive Bayes classification! This notebook has been carefully crafted to serve as a comprehensive companion for those who are taking their first steps into the world of machine learning. \n\n**If you're just starting out with machine learning,** this guide is designed specifically for you. We'll walk through the Naive Bayes classification technique in a way that's easy to understand, even if you're new to this exciting field ü§©.\n\n**By the time you finish this guide,** you'll have a solid grasp of how Naive Bayes works and how it can be used to make predictions and when you should use it üôå.\n\n**Let's dive in and unlock the power of Naive Bayes classification for beginners!**\n","metadata":{}},{"cell_type":"markdown","source":"# Na√Øve Bayes Classification : Spam Email Detection\n***Classifier to identify spam emails from legitimate ones***\n\n![classification](https://miro.medium.com/v2/resize:fit:4800/format:webp/1*_igArwmR7Pj_Mu_KUGD1SQ.png)\n\n","metadata":{}},{"cell_type":"markdown","source":"**What is Naive Bayes Classification?**\n\nIn 1763, the English statistician and philosopher Thomas Bayes proposed the Bayes Theorem, which serves as the fundamental principle of conditional probability. This theorem states that the likelihood of an event occurring, given the occurrence of another event, is equal to the conditional probability of the second event given the first event, multiplied by the probability of the first event itself.\n\nNaive Bayes is a popular classification approach that is rooted in Bayes' theory. The posterior class probability of a test data point can be calculated using class-conditional density estimation and class prior probability. The test data will then be assigned to the class with the highest posterior class probability.\n![Bayes Theorem](https://miro.medium.com/v2/resize:fit:4800/format:webp/1*CnoTGGO7XeUpUMeXDrIfvA.png)\n\n<!-- ![Bayes Theorem](https://images.app.goo.gl/7qNoQuk5iEn1gYFZ6) -->\n\n","metadata":{}},{"cell_type":"code","source":"# Try it out !\n# Uncomment the code to run this cell\n# Code Below\n# Calculating Conditional Probability using Bayes Theorem :-\n#---------------------------------------------------------------------------------------------------------------------------\n\n# P_A = float(input(\"Enter the probability of event A =  \"))  # Probability of event A\n# P_B_given_A = float(input(\"Enter the probability of event B given event A =  \"))  # Probability of event B given event A\n\n# # Calculate the complement of event A\n# P_not_A = 1 - P_A\n\n# # Calculate the probability of event B\n# P_B = P_B_given_A * P_A + (1 - P_B_given_A) * P_not_A\n\n# # Calculate the conditional probability using Bayes' theorem\n# P_A_given_B = (P_B_given_A * P_A) / P_B\n\n# # Print the results\n# print(f\"P(A|B) = {P_A_given_B:.2f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-03-31T14:20:24.280375Z","iopub.execute_input":"2024-03-31T14:20:24.28084Z","iopub.status.idle":"2024-03-31T14:20:24.325197Z","shell.execute_reply.started":"2024-03-31T14:20:24.280806Z","shell.execute_reply":"2024-03-31T14:20:24.324182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Why is it called Naive Bayes ?**\n\nThis classification methodology makes a naive assumption that features are independent of each other. For instance, consider the [Titanic Dataset](https://www.kaggle.com/datasets/brendan45774/test-file). When classifying using naive Bayes, we assume that data labels like age, gender, class, and cabin are all independent of each other.\n\nIt's important to note that this assumption is made to simplify our task, but in reality, these features may or may not be interrelated. The assumption helps us handle the complexity of the model, even though real-world data relationships might be more intricate.\n","metadata":{}},{"cell_type":"markdown","source":"**When should we consider using Naive Bayes?**\n\nIf we are given a condition where there are multiple events occouring at the same time and it is difficult to handle/understand tehm all at once, in such a case we may make an naive assumption considering all the events to be independent of each other. this would help us solve the problem with simplicity and comparatively less effort.\n","metadata":{}},{"cell_type":"markdown","source":"**Where can we use Naive bayes Classification ?**\n\nA few interesting projects could be :\n1. Spam Detection\n2. Character Recognition \n3. Whether Prediction\n4. News Article Catagorization\n5. Face Detection\n","metadata":{}},{"cell_type":"markdown","source":"**What are the types of Naive Bayes Classification?**\n\n1. **Bernoulli:** The Bernoulli model is suitable when our feature vectors are binary, meaning they can only take two values (usually 0 and 1). In the context of text classification with a 'bag of words' model, the 1s represent \"word occurs in the document,\" and the 0s represent \"word does not occur in the document.\" This model is useful when we want to represent presence or absence of certain features in our data.\n\n2. **Gaussian:** In classification, Gaussian is a method that assumes the features we use to describe data (like measurements or characteristics) follow a normal distribution. This means that most of the data points cluster around the average value, and fewer data points deviate far from this average.\n\n3. **Multinomial:** Multinomial is used when we are dealing with discrete counts. For example, in text classification, instead of just checking if a word occurs in a document (like in Bernoulli), we now count how many times a word appears in the document. It's like counting how many times a specific outcome (word) is observed over several trials (words in the document).\n\n\n\n","metadata":{}},{"cell_type":"markdown","source":"**Why is Naive Bayes a better classifier ?**\n\nNaive Bayes is a superior classifier because it employs probabilities to make predictions. Unlike other classifiers that rely on manually coded rules, Naive Bayes considers multiple features together, which makes it more accurate, especially with complex and large datasets.\n\nNaive Bayes uses probabilistic methods; thus, it can adapt to changes in data over time, giving it an edge over classifiers that struggle to maintain and update their fixed rules.","metadata":{}},{"cell_type":"markdown","source":"# Project :\n### Spam Email Detection \nUtilizing machine learning techniques to build a robust and efficient spam email detection system by implementing naive bayes classifier.\n\n**Github** : https://github.com/Satarupa22-SD/Spam_Detection (feel free to fork and use the project)","metadata":{}},{"cell_type":"code","source":"#Importing the Necessary libraries\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-31T14:20:24.327619Z","iopub.execute_input":"2024-03-31T14:20:24.328442Z","iopub.status.idle":"2024-03-31T14:20:25.08694Z","shell.execute_reply.started":"2024-03-31T14:20:24.328396Z","shell.execute_reply":"2024-03-31T14:20:25.084701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reading the data from .csv file\ndata = pd.read_csv('/kaggle/input/email-spam-detection-dataset-classification/spam.csv', encoding='latin-1')","metadata":{"execution":{"iopub.status.busy":"2024-03-31T14:20:25.088624Z","iopub.execute_input":"2024-03-31T14:20:25.090056Z","iopub.status.idle":"2024-03-31T14:20:25.147291Z","shell.execute_reply.started":"2024-03-31T14:20:25.090003Z","shell.execute_reply":"2024-03-31T14:20:25.146103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#display the first 5 rows\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-31T14:20:25.148789Z","iopub.execute_input":"2024-03-31T14:20:25.149223Z","iopub.status.idle":"2024-03-31T14:20:25.175898Z","shell.execute_reply.started":"2024-03-31T14:20:25.149189Z","shell.execute_reply":"2024-03-31T14:20:25.174636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Displaying the first 5 rows gives us an idea of how the data is arranged in the table. This helps us estimate which features are necessary and which are not.\n> Note : The label `ham` denotes non spam emails.","metadata":{}},{"cell_type":"markdown","source":"### Data Preprocessing \n\n#### What is Data Preprocessing ?\n\nIn simpler terms, data preprocessing refers to cleaning of the data. It is like chopping and cleaning the veggies before cooking them.\n\n***Defination :*** \n*Data preprocessing is the act of cleaning, converting, and organizing raw data such¬†that it may be fed into a machine learning or data analysis algorithm¬†in a more useable and structured shape.*\n\n\n#### why is it necessary ?\n\n**Quality assurance:** Raw data may have errors, inconsistencies, or missing numbers. By addressing these challenges, preprocessing ensures data quality.\n\n**Better Results:** Accurate, dependable insights are generated by good data. Clean, well-organized data helps algorithms function better.\n\n**Feature Engineering:** By combining existing features, you can construct new, useful ones that improve the model's capacity to grasp the data.\n\n**Reduced Noise:** Outliers or extreme values might cause results to be distorted. Preprocessing assists in identifying and dealing with them.\n\n**Standardization:** Different data sources may have varying units or scales. Data is more similar after preprocessing.\n\n**Missing Values:** Algorithms may struggle to handle missing values. Preprocessing aids in the filling or removal of missing data.\n\n**Efficiency:** Preparing data correctly saves time and computational resources during analysis.","metadata":{}},{"cell_type":"code","source":"# Drop the columns with NaN values\ndata = data.drop(columns=['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-03-31T14:20:25.179439Z","iopub.execute_input":"2024-03-31T14:20:25.179795Z","iopub.status.idle":"2024-03-31T14:20:25.190893Z","shell.execute_reply.started":"2024-03-31T14:20:25.179765Z","shell.execute_reply":"2024-03-31T14:20:25.189538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> We do not need the Not-a-Number (NaN) values, as they do not provide any insights into the data or impact other features. Therefore, we are dropping them.","metadata":{}},{"cell_type":"code","source":"# Rename columns for clarity:\ndata.columns = ['label', 'text']","metadata":{"execution":{"iopub.status.busy":"2024-03-31T14:20:25.192455Z","iopub.execute_input":"2024-03-31T14:20:25.193529Z","iopub.status.idle":"2024-03-31T14:20:25.204364Z","shell.execute_reply.started":"2024-03-31T14:20:25.19348Z","shell.execute_reply":"2024-03-31T14:20:25.203001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Displaying the first 5 rows to get basic understanding of the data\nprint(data.head())","metadata":{"execution":{"iopub.status.busy":"2024-03-31T14:20:25.206089Z","iopub.execute_input":"2024-03-31T14:20:25.206543Z","iopub.status.idle":"2024-03-31T14:20:25.219581Z","shell.execute_reply.started":"2024-03-31T14:20:25.206503Z","shell.execute_reply":"2024-03-31T14:20:25.217866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Separate Features and Target Labels\nA typical dataset consists of input features and corresponding target labels. The input features are the attributes or variables that are used to make predictions, while the target labels are the values we are trying to predict.\n\n*let us consider a simple example*\nImagine you're trying to teach a computer to tell whether a fruit is an apple or an orange based on its color and size. In this case,**Features** are the color and size of the fruit and **Labels** are whether the fruit is an apple or an orange.\n\n![Apple or Orange](https://assenjekov.com/wp-content/uploads/2015/05/apple-orange.jpg)","metadata":{}},{"cell_type":"markdown","source":"### Key Terms :\n\n* **train_test_split:** This function from the sklearn.model_selection module is used to split the data into training and testing sets.\n* **X_train:** This variable holds the subset of input features that will be used for training the model.\n* **X_test:** This variable holds the subset of input features that will be used for testing the model.\n* **y_train:** This variable holds the corresponding target labels for the training set.\n* **y_test:** This variable holds the corresponding target labels for the testing set.\n* **test_size=0.2:** This parameter indicates that 20% of the data will be allocated for testing, and the remaining 80% will be used for training.\n* **random_state=42:** This parameter is used to seed the random number generator, ensuring that the data is split in a reproducible manner. Using the same seed will produce the same split each time you run the code.","metadata":{}},{"cell_type":"code","source":"# Separate features (X) and target labels (y)\nX =  data.drop('label', axis=1)\ny = data['label']\n\n# Split the data into training and testing sets (80% training, 20% testing)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-03-31T14:20:25.221468Z","iopub.execute_input":"2024-03-31T14:20:25.222383Z","iopub.status.idle":"2024-03-31T14:20:25.239739Z","shell.execute_reply.started":"2024-03-31T14:20:25.222335Z","shell.execute_reply":"2024-03-31T14:20:25.238172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train the Classifier (Multinomial Naive Bayes)","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\n     ","metadata":{"execution":{"iopub.status.busy":"2024-03-31T14:20:25.241794Z","iopub.execute_input":"2024-03-31T14:20:25.24317Z","iopub.status.idle":"2024-03-31T14:20:25.250315Z","shell.execute_reply.started":"2024-03-31T14:20:25.243116Z","shell.execute_reply":"2024-03-31T14:20:25.249162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Why are we performing Count vectorization ?**\n\nWe're utilizing the **MultinomialNB()** classifier for this project, which exclusively accepts numeric values. However, our X_train and X_test datasets comprise text data (email messages). This is where **CountVectorizer()** comes in. It is being used here to convert the provided text into a vector, considering the frequency (count) of each word appearing throughout the entire text. This transformation is essential to enable the classifier to work with the text data effectively.","metadata":{}},{"cell_type":"code","source":"# Create a CountVectorizer instance\nvectorizer = CountVectorizer()","metadata":{"execution":{"iopub.status.busy":"2024-03-31T14:20:25.252391Z","iopub.execute_input":"2024-03-31T14:20:25.253961Z","iopub.status.idle":"2024-03-31T14:20:25.266805Z","shell.execute_reply.started":"2024-03-31T14:20:25.253885Z","shell.execute_reply":"2024-03-31T14:20:25.265561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fit and transform the training data (X_train)\nX_train_vectorized = vectorizer.fit_transform(X_train['text'])\n\n# Transform the test data (X_test)\nX_test_vectorized = vectorizer.transform(X_test['text'])","metadata":{"execution":{"iopub.status.busy":"2024-03-31T14:20:25.269021Z","iopub.execute_input":"2024-03-31T14:20:25.26995Z","iopub.status.idle":"2024-03-31T14:20:25.459477Z","shell.execute_reply.started":"2024-03-31T14:20:25.269902Z","shell.execute_reply":"2024-03-31T14:20:25.458023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the Multinomial Naive Bayes classifier\nclassifier = MultinomialNB()\nclassifier.fit(X_train_vectorized, y_train)","metadata":{"execution":{"iopub.status.busy":"2024-03-31T14:20:25.460896Z","iopub.execute_input":"2024-03-31T14:20:25.461326Z","iopub.status.idle":"2024-03-31T14:20:25.502729Z","shell.execute_reply.started":"2024-03-31T14:20:25.461292Z","shell.execute_reply":"2024-03-31T14:20:25.501107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Make Predictions on the Test Data\n\nIn this step, we are predicting the accuracy of our model by evaluating how precisely it can predict outcomes on new, unseen data.","metadata":{}},{"cell_type":"code","source":"# Make predictions on the test data\ny_pred = classifier.predict(X_test_vectorized)\n\n# Evaluate the model\naccuracy = accuracy_score(y_test, y_pred)\nconf_matrix = confusion_matrix(y_test, y_pred)\nclassification_rep = classification_report(y_test, y_pred)\n\nprint(f\"Accuracy: {accuracy:.2f}\")\nprint(\"Confusion Matrix:\")\nprint(conf_matrix)\nprint(\"Classification Report:\")\nprint(classification_rep)","metadata":{"execution":{"iopub.status.busy":"2024-03-31T14:20:25.504124Z","iopub.execute_input":"2024-03-31T14:20:25.504782Z","iopub.status.idle":"2024-03-31T14:20:25.598737Z","shell.execute_reply.started":"2024-03-31T14:20:25.504741Z","shell.execute_reply":"2024-03-31T14:20:25.597148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualizing the Data \n\nUnderstanding raw numbers or datasets can often be challenging. Therefore, it is crucial to visually represent our data. By generating visual representations of data, complex patterns, trends, and relationships become easier to comprehend than when dealing with raw numbers alone. Visualization also aids in identifying anomalies within the data. In the code snippet below, we have visualized the data using a histogram that displays the distribution of spam and non-spam emails.\n","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Count the number of spam and non-spam emails in the test set\nspam_counts = y_test.value_counts()\n\n# Plot the histogram\nplt.figure(figsize=(8, 6))\nplt.bar(spam_counts.index, spam_counts.values, color=['green', 'red'])\nplt.xlabel('Email Type')\nplt.ylabel('Number of Emails')\nplt.title('Number of Spam and Non-Spam Emails')\nplt.xticks([0, 1], ['ham (Non-Spam)', 'spam'])\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-03-31T14:20:25.605651Z","iopub.execute_input":"2024-03-31T14:20:25.60629Z","iopub.status.idle":"2024-03-31T14:20:25.930164Z","shell.execute_reply.started":"2024-03-31T14:20:25.606237Z","shell.execute_reply":"2024-03-31T14:20:25.929184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Reference :\n\n    [1] Ren, Jiangtao, Sau-dan. Lee, Xianlu Chen, Ben Kao, Reynold Cheng and David Wai-Lok Cheung. ‚ÄúNaive Bayes Classification of Uncertain Data.‚Äù 2009 Ninth IEEE International Conference on Data Mining (2009): 944-949.\n    [2] Jamshed, Humaira, M. S. A. Khan, Muhammad Khurram, Syed Inayatullah and Sameen Athar. ‚ÄúData Preprocessing: A preliminary step for web data mining.‚Äù 3C Tecnolog√≠a_Glosas de innovaci√≥n aplicadas a la pyme (2019): n. pag.\n    [3][CountVectorizer](http://https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)","metadata":{}}]}