{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 'Kaggle' competition #2\n",
    "\n",
    "## Classification\n",
    "\n",
    "You've learned a great deal about supervised learning and now it's time to bring together all that you've learned. You will be competing in a 'Kaggle Competition' along with the rest of the class! Your goal is to predict the target variable 'target' based on the features provided.  While you will be asked to take certain steps along the way to your submission, you're encouraged to try creative solutions to this problem and your choices are wide open for you to make your decisions on how to best make the predictions.\n",
    "\n",
    "\n",
    "**The Data**. The dataset provided is a generated dataset based on simulated data.  I've provided a training set and a testing set.  \n",
    "\n",
    "**Scoring**. You will need to achieve a minimum acceptable level of performance to demonstrate proficiency with using these supervised learning techniques.  As always, an additional 10 points are allocated to presentation quality. Beyond that, your grade will depend on the following:\n",
    "\n",
    "\n",
    "### Requirements:\n",
    "\n",
    "**(a) Explore your data.** Review and understand your data. Look at it; read up on what the features represent; think through the application domain; visualize statistics from the paper data to understand any key relationships. **There is no output required for this question**, but you are encouraged to explore the data personally before going further.\n",
    "\n",
    "**(b) Preprocess your data.** Preprocess your data so it's ready for use for classification and describe what you did and why you did it. Preprocessing may include: normalizing data, handling missing or erroneous values, separating out a validation dataset, preparing categorical variables through one-hot-encoding, etc. To make one step in this process easier, you're provided with a one-hot-encoded version of the data already. \n",
    "- Comment on each type of preprocessing that you apply and both how and why you apply it.\n",
    "\n",
    "**(c) Select, train, and compare models.** Fit at least 5 models to the data. Some of these can be experiments with different hyperparameter-tuned versions of the same model, although all 5 should not be the same type of model. There are no constraints on the types of models, but you're encouraged to explore examples we've discussed in class including:\n",
    "\n",
    "1. Logistic regression\n",
    "2. K-nearest neighbors\n",
    "3. Random Forests\n",
    "4. Neural networks\n",
    "5. Support Vector Machines\n",
    "6. Ensembles of models (e.g. model bagging, boosting, or stacking). `Scikit-learn` offers a number of tools for assisting with this including those for [bagging](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier), [boosting](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html), and [stacking](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html). You're also welcome to explore options beyond the `sklean` universe; for example, some of you may have heard of [XGBoost](https://github.com/dmlc/xgboost) which is a very fast implementation of gradient boosted decision trees that also allows for parallelization. \n",
    "\n",
    "When selecting models, be aware that some models may take far longer than others to train. Monitor your output and plan your time accordingly. \n",
    "\n",
    "Assess the classification performance AND computational efficiency of the models you selected:\n",
    "- Plot the ROC curves and PR curves for your models in two plots: one of ROC curves and one of PR curves. For each of these two plots, compare the performance of the models you selected above and trained on the training data, evaluating them on the validation data. Be sure to plot the line representing random guessing on each plot. One of these models should also be your BEST performing submission (based on evalutation using your test dataset). In the legends of each, include the area under the curve for each model. \n",
    "- As you train and validate each model time how long it takes to train and validate in each case and create a plot that shows both the training and prediction time for each model included in the ROC and PR curves.\n",
    "- Describe: \n",
    "  - Your process of model selection and hyperparameter tuning\n",
    "  - Which model performed best and your process for identifying/selecting it\n",
    "\n",
    "**(d) Apply your model \"in practice\".** Compare at least 5 different model results (using the test data provided). These do not need to be the same that you report on above, but you should select your *most competitive* models.\n",
    "- Be sure to RETRAIN YOUR MODEL ON ALL LABELED TRAINING AND VALIDATION DATA before making your predictions on the test data for submission. This will help to maximize your performance on the test data.\n",
    "\n",
    "\n",
    "### Guidance:\n",
    "1. **Preprocessing**. You may need to preprocess the data for some of these models to perform well (scaling inputs or reducing dimensionality). Some of this preprocessing may differ from model to model to achieve the best performance. A helpful tool for creating such preprocessing and model fitting pipelines is the sklearn `pipeline` module which lets you group a series of processing steps together.\n",
    "2. **Hyperparameters**. Hyperparameters may need to be tuned for some of the model you use. You may want to perform hyperparameter tuning for some of the models. If you experiment with different hyperparameters that include many model runs, you may want to apply them to a small subsample of your overall data before running it on the larger training set to be time efficient (if you do, just make sure to ensure your selected subset is representative of the rest of your data).\n",
    "3. **Validation data**. You're encouraged to create your own validation dataset for comparing model performance; without this, there's a significant likelihood of overfitting to the data. A common choice of the split is 80% training, 20% validation. Before you make your final predictions on the test data, be sure to retrain your model on the entire dataset.\n",
    "4. **Training time**. This is a larger dataset than you've worked with previously in this class, so training times may be higher that what you've experienced in the past. Plan ahead and get your model pipeline working early so you can experiment with the models you use for this problem and have time to let them run. \n",
    "\n",
    "### Starter code\n",
    "Below is some code for once you have predictions in the form of confidence scores for those classifiers, to produce submission files to send to me."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "################################\n",
    "# Produce submission\n",
    "################################\n",
    "\n",
    "def create_submission(confidence_scores, save_path):\n",
    "    '''Creates an output file of submissions for Kaggle\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    confidence_scores : list or numpy array\n",
    "        Confidence scores (from predict_proba methods from classifiers) or\n",
    "        binary predictions (only recommended in cases when predict_proba is \n",
    "        not available)\n",
    "    save_path : string\n",
    "        File path for where to save the submission file.\n",
    "    \n",
    "    Example:\n",
    "    create_submission(my_confidence_scores, './data/submission.csv')\n",
    "\n",
    "    '''\n",
    "    import pandas as pd\n",
    "\n",
    "    submission = pd.DataFrame({\"score\":confidence_scores})\n",
    "    submission.to_csv(save_path, index_label=\"id\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
