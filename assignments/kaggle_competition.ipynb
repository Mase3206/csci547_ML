{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle competition #2\n",
    "\n",
    "\n",
    "You've learned a great deal about supervised learning and now it's time to bring together all that you've learned. You will be competing in a 'Kaggle Competition' along with the rest of the class! Your goal is to predict the 'target' based on the features provided.  While you will be asked to take certain steps along the way to your submission, you're encouraged to try creative solutions to this problem and your choices are wide open for you to make your decisions on how to best make the predictions.\n",
    "\n",
    "The competition is hosted [here](https://www.kaggle.com/t/1186963c10534692a4148b7c9528149d) and the competition will be open from 11/7 at noon and will stay open until 11/22 at 11pm.  Please follow the directions on kaggle to submit your predictions. \n",
    "\n",
    "\n",
    "**The Data**. This is a regression problem with 20 numerical predictors and 1 target. You have access to 400 training samples, and there are 800 testing samples. You will train your model on the training samples and then make predictions on the 800 testing samples, which you will submit (as a .csv file) to be scored. There is an example testing submission file included to demonstrate how the submission files should look.\n",
    "\n",
    "**Scoring**. Your performance will be judged using mean-squared error (MSE).  You will submit your predictions to kaggle using the submission instructions. The leaderboard will show your performance on half of the testing samples (400 samples), while another 400 remain for \"private\" testing, and your score on this private set will be revealed at the end of the competition. You will need to achieve a minimum acceptable level of performance to demonstrate proficiency with using these supervised learning techniques.  \n",
    "\n",
    "Beyond that, it's an open competition and scoring in the top three places of the private leaderboard will result in \\textbf{5 bonus points in this assignment} (and the pride of the class!). Note: the Kaggle leaderboard has a public and private component. The public component is viewable throughout the competition, but the private leaderboard is revealed at the end. When you make a submission, you immediately see your submission on the public leaderboard, but that only represents scoring on a fraction of the total collection of test data, the rest remains hidden until the end of the competition to prevent overfitting to the test data through repeated submissions. You will be be allowed to hand-select two eligible submissions for private score, or by default your best two public scoring submissions will be selected for private scoring.\n",
    "\n",
    "**Grade** Your grade on this will be based on both your performance (50 points) and the notebook you submit (40 points).  As always, an additional 10 points are allocated to presentation quality. For your notebook, your grade will depend on the following:\n",
    "\n",
    "### Requirements:\n",
    "\n",
    "**(a) Explore your data.** Review and understand your data. Look at it; use plots to help understand any key relationships. **There is no specific output required for this question**, but you are encouraged to explore the data personally before going further.\n",
    "\n",
    "**(b) Preprocess your data.** Preprocess your data so it's ready for use for regression and describe what you did and why you did it. Preprocessing may include: normalizing data, handling missing or erroneous values, separating out a validation dataset, etc.\n",
    "- Comment on each type of preprocessing that you apply and both how and why you apply it.\n",
    "\n",
    "**(c) Select, train, and compare models.** Fit at least 5 models to the training data. Some of these can be experiments with different hyperparameter-tuned versions of the same model, although all 5 should not be the same type of model. There are no constraints on the types of models, but you're encouraged to explore examples we've discussed in class including:\n",
    "\n",
    "1. Linear regression\n",
    "2. Polynomial regression\n",
    "3. Lasso regression\n",
    "4. Ridge regression\n",
    "5. Decision tree regression\n",
    "6. Random forest regression\n",
    "7. Support vector regression (we didn't explicitly cover this in class)\n",
    "\n",
    "\n",
    "When selecting models, be aware that some models may take longer than others to train. Monitor your output and plan your time accordingly. \n",
    "\n",
    "Assess the regression performance AND computational efficiency of the models you selected:\n",
    "\n",
    "- Visualize the model fit to the training data. Using the models you created in part (c), plot the original data (as a scatter plot) AND the curves representing your models (each as a separate curve) from (c). One of these models should also be your BEST performing submission (based on evalutation using your test dataset). Label it as such. \n",
    "- As you train and validate each model, time how long it takes to train and validate in each case and create a plot that shows both the training and prediction time for each model included. \n",
    "- Describe: \n",
    "  - Your process of model selection and hyperparameter tuning\n",
    "  - Which model performed best and your process for identifying/selecting it\n",
    "\n",
    "**(d) Apply your model \"in practice\".** Compare at least 5 different model results (using the test data provided). These do not need to be the same that you report on above, but you should select your *most competitive* models.\n",
    "- Be sure to RETRAIN YOUR MODEL ON ALL LABELED TRAINING AND VALIDATION DATA before making your predictions on the test data for submission. This will help to maximize your performance on the test data.\n",
    "\n",
    "\n",
    "\n",
    "### Guidance:\n",
    "1. **Preprocessing**. You may need to preprocess the data for some of these models to perform well (scaling inputs or reducing dimensionality). Some of this preprocessing may differ from model to model to achieve the best performance. A helpful tool for creating such preprocessing and model fitting pipelines is the sklearn `pipeline` module which lets you group a series of processing steps together.\n",
    "2. **Hyperparameters**. Hyperparameters may need to be tuned for some of the models you use. You may want to perform hyperparameter tuning for some of the models. `optuna` is a great framework for this, if you want to try it out. If you experiment with different hyperparameters that include many model runs, you may want to apply them to a small subsample of your overall data before running it on the larger training set to be time efficient (if you do, just make sure to ensure your selected subset is representative of the rest of your data).\n",
    "3. **Validation data**. You're encouraged to create your own validation dataset for comparing model performance; without this, there's a significant likelihood of overfitting to the data. A common choice of the split is 80% training, 20% validation. Before you make your final predictions on the test data, be sure to retrain your model on the entire dataset.\n",
    " "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
